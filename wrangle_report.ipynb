{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting: Data_Wrangling_Project_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are performing Data Wrangling on the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comments about the dog.\n",
    "\n",
    "This data have some quality and tidiness issues that needs to be identified and clean by following the data wrangling process for the data to be fit for analysis. \n",
    "\n",
    "At the end of the project:\n",
    "1. Identify at least 8 quality issues and 3 tidiness issues\n",
    "2. Clean the identified issues\n",
    "3. Perform some analysis and bring out at least 3 insights and 1 visualization from the cleaned data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of data wrangling is to gather the required data to be cleaned. This involves collating the data from different sources either manually or programmatically.\n",
    "\n",
    "For this project, we used 3 separate data, each of which was gathered separately.\n",
    "\n",
    "1. **Twitter_archive_enhanced data**: This data was downloaded manually, upload to the project workspace, and read into pandas DataFrame.\n",
    "2. **The tweet image_predictions**: This data was gathered programmatically by using the Request library and the given URL. It is a .tsv file. It was downloaded programmatically into the workspace and read into pandas DataFrame with tabular separation (\\t)\n",
    "3. **Tweet_json data**: This required querying Twitter API data using the Tweepy library and stored in tweet_json.txt.\n",
    "> I used the provided `twitter_api.py` code and `tweet_json.txt` due to my inability to secure access to the Twitter API data after waiting for a day. So, I used the provided one and I opened and read the JSON text file to extract the needed column (tweet_id, favorite_count, and retweet count) to be stored in a pandas dataframe called `tweet_json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assessed the 3 data visually and programmatically using `(.info(), .describe(), .duplicated(), .isna(), .value_counts())` methods.\n",
    "\n",
    "After assessing the data, the following quality and tidiness issues were identified:\n",
    "\n",
    "##### Quality issues\n",
    " \n",
    "**twitter_archive_enhanced**\n",
    "\n",
    "1. The null value on the doggo, floofer, pupper, and puppo column are recorded as none\n",
    "\n",
    "2. Name of some dogs were not properly captured or not captured at all as some dogs were given names such as a, an, such, quite. Other unavailable names are recorded as None\n",
    "\n",
    "3. There are 181 retweets data in the datasets to be removed\n",
    "\n",
    "4. Some columns are not useful for analysis, most of it even contains large missing data (source, in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, expanded url)\n",
    "\n",
    "5. The rating denominator has some values not equal to 10 as 10 is the standard denominator\n",
    "\n",
    "6. The rating numerator has some extreme values which seems unrealistic, like (420, 666, 182, 1776)\n",
    "\n",
    "7. Other rating numerator were incorrectly captured due to having multiples of the numerators in tens or the decimal part of a decimal ratings\n",
    "\n",
    "8. Timestamp was recorded as object instead of datetime datatype and tweet_id is int instead of string\n",
    "\n",
    "**image_predictions**\n",
    "\n",
    "9. tweet_id is int instead of string\n",
    "\n",
    "10. jpg_url has 66 duplicated values\n",
    "\n",
    "11. The predicted dog breed is not well defined, someone can struggle to know what p1, p2, or p3 is\n",
    "\n",
    "12. The dog breed should have a consistence case format so that repeated breed can be capture without issue\n",
    "\n",
    "**tweet_json_df**\n",
    "\n",
    "13. tweet_id is int instead of string\n",
    "\n",
    "\n",
    "##### Tidiness issues\n",
    "\n",
    "1. **twitter_archive_enhanced** - The classes of the dogs (doggo, floofer, pupper, puppo) was recorded in separate column instead of one column for the class of dogs and some dogs have more than one classification\n",
    "\n",
    "2. **image_predictions** - Prediction 1 has the highest confidence level, then we can drop other predictions with their correspondence.\n",
    "\n",
    "3. The three datasets can be combined into one data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I performed cleaning following the established way of cleaning identified issues by defining, coding, and testing. Each of the issues was properly addressed with defined cleaning methods and properly tested.\n",
    "\n",
    "After cleaning all the quality and tidiness issues, the 3 datasets was merged and stored in `twittwe_archive_master` in a CSV file. This master data was later used for analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
